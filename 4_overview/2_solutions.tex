\section{Существующие решения.}

\subsection{Итеративный алгоритм ближайших точек}

Итеративный алгоритм ближайших точек (Iterative Closest Points, ICP) \cite{icp}
состоит из двух чередующихся операций.
Инициализируется алгоритм единичной матрицей поворота $R = I$
и нулевым вектором смещения $ \vec{b} = \vec{0}$.
Первая итерация состоит в поиске такой разметки $k \, : \, S \to T$, чтобы
\begin{equation*}
 \sum \limits_{s \in S}
  \left \Vert R \cdot \vec{s} + \vec{b} - \vec{k}_s \right \Vert^2 \to
  \min \limits_{k},
\end{equation*}
где $R$ и $ \vec{b}$ фиксированы.
Функция $k$~---~это множество упорядоченных пар $ \left( s, t \right) \in S \times T$, таких,
что пары существуют для всех элементов множества $S$, и, если первые элементы пар совпадают,
то совпадают и вторые элементы.
Тогда можем искать такой набор $ \left\{ \vec{k_s} \; \middle| \; \vec{s} \in S \right\} $, чтобы
\begin{equation*}
  \sum \limits_{s \in S} \left \Vert R \vec{s} + \vec{b} - \vec{k_s} \right \Vert^2 \to
  \min \limits_{k_s}.
\end{equation*}
Запишем сумму явно (пусть множество $S$ содержит $n$ точек)
\begin{equation*}
  \left \Vert R \cdot \vec{s}_1 + \vec{b} - \vec{k}_{s_1} \right \Vert^2 +
  \left \Vert R \cdot \vec{s}_2 + \vec{b} - \vec{k}_{s_2} \right \Vert^2 + \dotsc +
  \left \Vert
    R \cdot \vec{s}_n + \vec{b} - \vec{k}_{s_n}
  \right \Vert^2 \to
  \min \limits_{k_{s_1}, k_{s_2}, \dotsc, k_{s_n} \in T}.
\end{equation*}
Параметры, которые входят в каждое слагаемое, разные, так что
\begin{align*}
  \left\{ \begin{array}{ccc}
    \left \Vert R \cdot \vec{s}_1 + \vec{b} - \vec{k}_{s_1} \right \Vert^2
    \to \min \limits_{k_{s_1} \in T}, \\
    \left \Vert R \cdot \vec{s}_2 + \vec{b} - \vec{k}_{s_2} \right \Vert^2 \to
    \min \limits_{k_{s_2} \in T}, \\
    \vdots \\
    \left \Vert
      R \cdot \vec{s}_n + \vec{b} - \vec{k}_{s_n}
    \right \Vert^2 \to
    \min \limits_{k_{s_n} \in T}.
  \end{array} \right.
\end{align*}
Таким образом, для каждой точки $ \vec{s} \in S$ находим точку $ \vec{t} \in T$ такую,
чтобы расстояние между парами $R \cdot \vec{s} + \vec{b}$ и $ \vec{t}$ для всех $ \vec{s} \in S$
было наименьшим
\begin{equation*}
  \left \Vert R \cdot \vec{s} + \vec{b} - \vec{t} \right \Vert^2 =
  \min \limits_{t_i \in T} \left \Vert R \vec{s} + \vec{b} - \vec{t_i} \right \Vert^2.
\end{equation*}

На следующей итерации происходит поиск поворота $R$ и смещения $ \vec{b}$ при текущей разметке
$ \left\{ \vec{k}_s \; \middle| \; \vec{s} \in S \right\} $
\begin{equation}\label{eq:second:iteration}
 \sum \limits_{s \in S}
  \left \Vert R \cdot \vec{s} + \vec{b} - \vec{k}_s \right \Vert^2
 \to \min \limits_{R, b}.
\end{equation}
При этом матрица $R \in SO \left( 3 \right) $,
то есть ортогональна матрица размерности $3 \times 3$ с определителем $+1$,
которая в качестве линейного преобразования действует как поворот,
и пусть $ \left| S \right| = n < \infty $.

Вычислим смещение $ \vec{b}$.
Пусть $R$~---~фиксирована.
Минимизируем
\begin{equation*}
  E \left( \vec{b} \right) =
  \sum \limits_{s \in S} \left \Vert R \cdot \vec{s} + \vec{b} - \vec{k}_s \right \Vert^2.
\end{equation*}
Можем найти оптимальное смещение, взяв производную от $E$ по $ \vec{b}$ и приравняв её к нулю
\begin{equation}\label{eq:derivative}
  0 =
  \frac{dE}{d \vec{b}} =
  \sum \limits_{s \in S} 2 \left( R \cdot \vec{s} + \vec{b} - \vec{k}_s \right) =
  2 \vec{b} \cdot \left| S \right| +
  2 R \sum \limits_{s \in S} \vec{s} -
  2 \sum \limits_{s \in S} \vec{k}_s.
\end{equation}
Обозначим
\begin{equation*}
  \overline{s} = \frac{ \sum \limits_{s \in S} \vec{s}}{ \left| S \right| }, \,
  \overline{k}_s = \frac{ \sum \limits_{s \in S} \vec{k}_s}{ \left| S \right| }.
\end{equation*}
Перепишем \eqref{eq:derivative} в терминах введённых обозначений
\begin{equation}\label{eq:translation}
  \vec{b} =
  \overline{k}_s - R \cdot \overline{s}.
\end{equation}
Нашли оптимальный вектор $ \vec{b}$ для любой матрицы поворота $R$.
Подставим его в выражение \eqref{eq:second:iteration}
\begin{gather*}
  \sum \limits_{s \in S} \left \Vert R \cdot \vec{s} + \vec{b} - \vec{k}_s \right \Vert^2 =
  \sum \limits_{s \in S}
    \left \Vert
      R \cdot \vec{s} + \overline{k}_s - R \cdot \overline{s} - \vec{k}_s
    \right \Vert^2 = \\
  = \sum \limits_{s \in S}
    \left \Vert
      R \cdot \left( \vec{s} - \overline{s} \right) - \left( \vec{k}_s - \overline{k}_s \right)
    \right \Vert^2.
\end{gather*}
Таким образом, ищем оптимальный поворот $R$, переформулируя задачу так,
чтобы смещение было равно нулю.
Пусть
\begin{equation*}
  \tilde{s} = \vec{s} - \overline{s}, \,
  \tilde{k}_s = \vec{k}_s - \overline{k}_s,
\end{equation*}
тогда
\begin{equation}\label{eq:R}
  R =
  \arg \min \limits_{R \in SO \left( 3 \right) }
    \sum \limits_{s \in S} \left \Vert R \cdot \tilde{s} - \tilde{k}_s \right \Vert^2.
\end{equation}
Упростим выражение, которое минимизируем в \eqref{eq:R}
\begin{gather*}
  \left \Vert R \cdot \tilde{s} - \tilde{k}_s \right \Vert^2 =
  \left( R \cdot \tilde{s} - \tilde{k}_s \right)^T \left( R \cdot \tilde{s} - \tilde{k}_s \right) =
  \left( \tilde{s}^T \cdot R^T - \tilde{k}_s^T \right)
  \left( R \cdot \tilde{s} - \tilde{k}_s \right) = \\
  = \tilde{s}^T \cdot R^T \cdot R \cdot \tilde{s} -
  \tilde{k}_s \cdot \tilde{s}^T \cdot R \cdot \tilde{s} -
  \tilde{s}^T \cdot R^T \cdot \tilde{k}_s + \tilde{k}_s^T \cdot \tilde{k}_s = \\
  = \tilde{s}^T \cdot \tilde{s} - \tilde{k}_s^T \cdot R \cdot \tilde{s} -
  \tilde{s}^T \cdot R^T \cdot \tilde{k}_s + \tilde{k}_s^T \cdot \tilde{k}_s.
\end{gather*}
Использовали ортогональность матрицы $R$, то есть что $R^T \cdot R = I$~---~единичная матрица.

Заметим, что $ \tilde{s}^T \cdot R^t \cdot \tilde{k}_s $~---~это скаляр:
$ \tilde{s}^T$ имеет размерность $1 \times 3, \, R^T$ имеет размерность $3 \times 3$ и
$ \tilde{k}_s$~---~$3 \times 1$.
Для любого скаляра $a = a^T$, поэтому
\begin{equation*}
  \tilde{s}^T \cdot R^T \cdot \tilde{k}_s =
  \left( \tilde{s}^T \cdot R^T \cdot \tilde{k}_s \right)^T =
  \tilde{k}_s^T \cdot R \cdot \tilde{s}.
\end{equation*}
Имеем
\begin{equation*}
  \left \Vert R \cdot \tilde{s} - \tilde{k}_s \right \Vert^2 =
  \tilde{s}^T \cdot \tilde{s} - 2 \tilde{k}_s^T \cdot R \cdot \tilde{s} +
  \tilde{k}_s^T \cdot \tilde{k}_s.
\end{equation*}
Подставим полученное выражение в \eqref{eq:R}
\begin{gather*}
  R =
  \arg \min \limits_{R \in SO \left( 3 \right) }
    \sum \limits_{s \in S}
      \left(
        \tilde{s}^T \cdot \tilde{s} - 2 \tilde{k}_s^T \cdot R \cdot \tilde{s} +
        \tilde{k}_s^T \cdot \tilde{k}_s
      \right) = \\
  = \arg \min \limits_{R \in SO \left( 3 \right) }
    \left(
      \sum \limits_{s \in S} \tilde{s}^T \cdot \tilde{s} -
      2 \sum \limits_{s \in S} \tilde{k}_s^T \cdot R \cdot \tilde{s} +
      \sum \limits_{s \in S} \tilde{k}_s \cdot \tilde{k}_s
    \right) = \\
  = \arg \min \limits_{R \in SO \left( 3 \right) }
    \left( -2 \sum \limits_{s \in S} \tilde{k}_s^T \cdot R \cdot \tilde{s} \right).
\end{gather*}
Отбросили суммы $ \tilde{s}^T \cdot \tilde{s}$ и $ \tilde{k}_s^T \cdot \tilde{k}_s$
по всем $s \in S$, потому что эти выражения не зависят от $R$ и не влияют на минимизацию.
То же самое справедливо для константы, на которую умножается сумма, поэтому
\begin{equation*}
   R =
   \arg \max \limits_{R \in SO \left( 3 \right) }
    \sum \limits_{s \in S} \tilde{k}_s^T \cdot R \cdot \tilde{s}.
\end{equation*}
Заметим, что
\begin{equation*}
  \sum \limits_{s \in S} \tilde{k}_s^T \cdot R \cdot \tilde{s} =
  tr \left( \tilde{K}^T \cdot R \cdot \tilde{S} \right),
\end{equation*}
где $ \tilde{K}$ и $ \tilde{S}$ --- это матрицы размерности $3 \times n$
со столбцами $ \tilde{s}$ и $ \tilde{k}_s$ соответсвенно
\begin{gather*}
  \tilde{K}^T \cdot R \cdot \tilde{S} =
  \begin{bmatrix}
    \tilde{k}_{s_1}^T \\
    \tilde{k}_{s_2}^T \\
    \vdots \\
    \tilde{k}_{s_n}^T
  \end{bmatrix} \cdot R \cdot
  \begin{bmatrix}
    \tilde{s}_1 & \tilde{s}_2 & \dotsc & \tilde{s}_n
  \end{bmatrix}.
\end{gather*}
След квадратной матрицы равен сумме её диагональных элементов.
Ищем такую матрицу $R$,
которая будет максимизировать выражение $tr \left( \tilde{K}^T \cdot R \cdot \tilde{S} \right) $.
След матрицы имеет свойство \cite{trace:fukugana}
\begin{equation*}
  tr \left( A \cdot B \right) =
  tr \left( B \cdot A \right)
\end{equation*}
для любых матриц $A$ и $B$ совместимых размерностей.

Приведём доказательство этого свойства.
Пусть матрица $A$ имеет размерность $n \times m$, а матрица $B$~---~$m \times n$.
Тогда матрица $C = A \cdot B$~---~матрица размерности $n \times n$ состоит из элементов
\begin{equation*}
  c_{ij} =
  \sum \limits_{r = 1}^m a_{ir} \cdot b_{rj}.
\end{equation*}
Аналогично, матрица $D = B \cdot A$ имеет размерность $m \times m$ и состоит из элементов
\begin{equation*}
  d_{ij} =
  \sum \limits_{r = 1}^n b_{ir} \cdot a_{rj}.
\end{equation*}
Диагональные элементы матриц $C$ и $D$ имеют вид
\begin{equation*}
  c_{ii} = \sum \limits_{r = 1}^m a_{ir} \cdot b_{ri}, \,
  d_{ii} = \sum \limits_{r = 1}^n b_{ir} \cdot a_{ri}.
\end{equation*}
Запишем след для произведений
\begin{gather*}
  tr \left( A \cdot B \right) = tr \left( C \right) = \sum \limits_{i = 1}^n c_{ii} =
  \sum \limits_{i = 1}^n \sum \limits_{r = 1}^m a_{ir} \cdot b_{ri}, \\
  tr \left( B \cdot A \right) = tr \left( D \right) = \sum \limits_{i = 1}^m d_{ii} =
  \sum \limits_{i = 1}^m \sum \limits_{r = 1}^n b_{ir} \cdot a_{ri}.
\end{gather*}
Получившиеся двойные суммы одинаковы с точностью до переименования индексов суммирования.
Это означает, что $tr \left( A \cdot B \right) = tr \left( B \cdot A \right) $.

Таким образом,
\begin{equation*}
  tr \left( \tilde{K}^T \cdot R \cdot \tilde{S} \right) =
  tr \left( \tilde{K}^T \cdot \left( R \cdot \tilde{S} \right) \right) =
  tr \left( R \cdot \tilde{S} \cdot \tilde{K}^T \right).
\end{equation*}
Обозначим
\begin{gather*}
  X =
  \tilde{S} \cdot \tilde{K}^T =
  \sum \limits_{i = 1}^n \tilde{s}_i \cdot \tilde{k}_{s_i}^T =
  \sum \limits_{i = 1}^n
    \tilde{s}_i \cdot \left( \vec{k}_{s_i} - \overline{k}_s \right)^T = \\
  = \begin{bmatrix}
    \sum \limits_{i = 1}^n \tilde{s}_{ix} \cdot \tilde{k}_{s_i x} & \sum \limits_{i = 1}^n \tilde{s}_{ix} \cdot \tilde{k}_{s_i y} & \sum \limits_{i = 1}^n \tilde{s}_{ix} \cdot \tilde{k}_{s_i z} \\
    \sum \limits_{i = 1}^n \tilde{s}_{iy} \cdot \tilde{k}_{s_i x} & \sum \limits_{i = 1}^n \tilde{s}_{iy} \cdot \tilde{k}_{s_i y} & \sum \limits_{i = 1}^n \tilde{s}_{iy} \cdot \tilde{k}_{s_i z} \\
    \sum \limits_{i = 1}^n \tilde{s}_{iz} \cdot \tilde{k}_{s_i x} & \sum \limits_{i = 1}^n \tilde{s}_{iz} \cdot \tilde{k}_{s_i y} & \sum \limits_{i = 1}^n \tilde{s}_{iz} \cdot \tilde{k}_{s_i z}
  \end{bmatrix}.
\end{gather*}

Возьмём сингулярное разложение \cite{golub:svd} матрицы $X$
\begin{equation*}
  X = U \cdot \Sigma \cdot V^T,
\end{equation*}
где $U \in \mathbb{R}^{3 \times 3}$ и $V \in \mathbb{R}^{3 \times 3}$~---~ортогональные матрицы,
а $ \Sigma \in \mathbb{R}^{3 \times 3}$~---~диагональная матрица с неотрицательными элементами,
причём $ \sigma_1 \geq \sigma_2 \geq \sigma_3 \geq 0$.
Ранг матрицы $ \Sigma $ меньше трёх, если
$rank \, X =
  rank \left( \tilde{S} \cdot \tilde{K}^T \right) < 3$.
Такое возможно, если матрица $X$ имеет нулевой определитель,
то есть у неё есть столбец, который является линейной комбинацией остальных столбцов.

Раскроем этот определитель матрицы
\begin{gather*}
  \det{X} =
  \sum \limits_{i = 1}^n \tilde{s}_{ix} \cdot \tilde{k}_{s_i x} \cdot \sum \limits_{i = 1}^n \tilde{s}_{iy} \cdot \tilde{k}_{s_i y} \cdot \sum \limits_{i = 1}^n \tilde{s}_{iz} \cdot \tilde{k}_{s_i z} + \\
  + \sum \limits_{i = 1}^n \tilde{s}_{ix} \cdot \tilde{k}_{s_i y} \cdot \sum \limits_{i = 1}^n \tilde{s}_{iy} \cdot \tilde{k}_{s_i z} \cdot \sum \limits_{i = 1}^n \tilde{s}_{iz} \cdot \tilde{k}_{s_i x} + \\
  + \sum \limits_{i = 1}^n \tilde{s}_{ix} \cdot \tilde{k}_{s_i z} \cdot \sum \limits_{i = 1}^n \tilde{s}_{iy} \cdot \tilde{k}_{s_i x} \cdot \sum \limits_{i = 1}^n \tilde{s}_{iz} \cdot \tilde{k}_{s_i y} - \\
  - \sum \limits_{i = 1}^n \tilde{s}_{ix} \cdot \tilde{k}_{s_i x} \cdot \sum \limits_{i = 1}^n \tilde{s}_{iy} \cdot \tilde{k}_{s_i z} \cdot \sum \limits_{i = 1}^n \tilde{s}_{iz} \cdot \tilde{k}_{s_i y} - \\
  - \sum \limits_{i = 1}^n \tilde{s}_{ix} \cdot \tilde{k}_{s_i y} \cdot \sum \limits_{i = 1}^n \tilde{s}_{iy} \cdot \tilde{k}_{s_i x} \cdot \sum \limits_{i = 1}^n \tilde{s}_{iz} \cdot \tilde{k}_{s_i z} - \\
  - \sum \limits_{i = 1}^n \tilde{s}_{ix} \cdot \tilde{k}_{s_i z} \cdot \sum \limits_{i = 1}^n \tilde{s}_{iy} \cdot \tilde{k}_{s_i y} \cdot \sum \limits_{i = 1}^n \tilde{s}_{iz} \cdot \tilde{k}_{s_i x}.
\end{gather*}

Выпишем отдельно один элемент матрицы $X$
\begin{gather*}
  \sum \limits_{i = 1}^n \tilde{s}_{ix} \cdot \tilde{k}_{s_i x} =
  \sum \limits_{i = 1}^n \tilde{s}_{ix} \cdot \left( R \vec{s}_i + \vec{b} + \vec{ \xi }_{s_i} - \frac{ \sum \limits_{j = 1}^n \left( R \cdot \vec{s}_j + \vec{b} + \vec{ \xi }_{s_j} \right) }{n} \right)_x \sim \\
  \sim N \left(
    \sum \limits_{i = 1}^n
      \tilde{s}_{ix} \cdot \left( R \cdot \vec{s}_i + \vec{b} -
      \frac{ \sum \limits_{j = 1}^n \left( R \cdot \vec{s}_j + \vec{b} \right) }{n} \right)_x,
    \frac{ \sigma^2 \cdot I \cdot \left( n - 1 \right) }{n} \sum \limits_{i = 1}^n \tilde{s}_{ix}^2
  \right).
\end{gather*}
Это гауссовская случайная величина, то есть величина, имеющая непрерывное распределение.
Тогда $ \det{X}$~---~случайная величина с непрерывным распределением, и вероятность того,
что $ \det{X} = 0$, равна нулю.
Таким образом, матрица $ \Sigma $ будет иметь ранг 3 с вероятностью 1,
то есть имеет место неравенство $ \sigma_1 \geq \sigma_2 \geq \sigma_3 > 0$.

Подставим сингулярное разложение в след
\begin{equation*}
  tr \left( R \cdot X \cdot Y^T \right) =
  tr \left( R \cdot S \right) =
  tr \left( R \cdot U \cdot \Sigma \cdot V^T \right) =
  tr \left( \Sigma \cdot V^T \cdot R \cdot U \right).
\end{equation*}
Заметим, что $V, \, R$ и $U$~---~ортогональные матрицы,
поэтому матрица
\begin{equation*}
  M =
  V^T \cdot R \cdot U
\end{equation*}
также ортогональная.
Это означаает, что $ \vec{m}_i \cdot \vec{m}_i^T = 1$ для каждой строки $ \vec{m}_i$ матрицы $M$.
Следовательно, модули всех элементов $m_{ij}$ матрицы $M$ не превосходят единицы
\begin{equation*}
  1 =
  \vec{m}_i \cdot \vec{m}_i^T =
  \sum \limits_{j = 1}^3 m_{ij}^2 \Rightarrow
  m_{ij}^2 \leq 1 \Rightarrow
  \left| m_{ij} \right| \leq 1.
\end{equation*}
Вспомним, что $ \Sigma $ ---
диагональная матрица с неотрицательными элементами $ \sigma_1 \geq \sigma_2 \geq \sigma_3 \geq 0$.
Поэтому
\begin{equation*}
  tr \left( \Sigma \cdot M \right) =
  tr \left(
  \begin{bmatrix}
    \sigma_1 & 0 & 0 \\
    0 & \sigma_2 & 0 \\
    0 & 0 & \sigma_3
  \end{bmatrix} \cdot
  \begin{bmatrix}
    m_{11} & m_{12} & m_{13} \\
    m_{21} & m_{22} & m_{23} \\
    m_{31} & m_{32} & m_{33}
  \end{bmatrix}
  \right) =
  \sum \limits_{i = 1}^3 \sigma_i \cdot m_{ii} \leq
  \sum \limits_{i = 1}^3 \sigma_i.
\end{equation*}
Поэтому след максимизируется при $m_{ii} = 1 \qquad \forall i \in \left\{ 1, 2, 3 \right\} $.
Так как $M$ --- ортогональная матрица, то она должна быть единичной
\begin{equation*}
  I = M = V^T \cdot R \cdot U \Rightarrow
  V = R \cdot U \Rightarrow
  R = V \cdot U^T.
\end{equation*}

Заметим, что сейчас $R$ --- это ортогональная матрица, но при этом возможны две ситуации:
когда $ \det{R} = \det{ \left( V \cdot U^T \right) } = 1$,
то есть матрица $R$ действует как поворот, и $ \det{R} = \det{ \left( V \cdot U^T \right) } = -1$,
то есть матрица $R$ действует как поворот и отражение.
Предположим, что $ \det{ \left( V \cdot U^T \right) } = -1$.
Это эквивалентно тому, что $ \det{M} = -1$.
Ищем такую матрицу $M$,
которая максимизирует выражение
\begin{equation*}
  tr \left( \Sigma \cdot M \right) =
  \sigma_1 \cdot m_{11} + \sigma_2 \cdot m_{22} + \sigma_3 \cdot m_{33}.
\end{equation*}
Рассматриваем переменные $ \left( m_{11}, m_{22}, m_{33} \right) $.
Это множество всех диагоналей ортогональной матрицы порядка 3.
Альфред Хорн доказал \cite{horn:diagonal:rotation},
что вектор $ \left( d_1, \dotsc, d_n \right) $ ---
это диагональ матрицы поворота порядка $n$ тогда и только тогда,
когда он лежит в выпуклой оболочке точек $ \left( \pm 1, \dotsc, \pm 1 \right) $,
где чётное число значений (в том числе 0) равно $-1$.
Для нашего случая эта теорема принимает вид: $M$ --- матрица поворота тогда и только тогда,
когда её диагональ $ \left( m_{11}, m_{22}, m_{33} \right) $ лежит в выпуклой оболочке точек
$ \left( \pm 1, \pm 1, \pm 1 \right) $, где чётное число координат (в том числе 0) равно $-1$.
Матрица $M$ --- матрица поворота и отражения,
потому для неё оптимальная диагональ имеет вид $ \left( 1, 1, -1 \right) $,
когда нечётное число значений равно $-1$, соответственно,
\begin{equation*}
  tr \left( \Sigma \cdot M \right) = \sigma_1 + \sigma_2 - \sigma_3.
\end{equation*}
Это значение больше любого другого вектора из $ \left( \pm 1, \pm 1, \pm 1 \right) $
за исключением $ \left( 1, 1, 1 \right) $,
потому что $ \sigma_3$ --- это наименьшее сингулярное значение.

Таким образом, если $ \det{ \left( V \cdot U^T \right) } = -1$, то
\begin{equation*}
  M =
  V^T \cdot R \cdot U =
  \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & -1
  \end{bmatrix} \Rightarrow
  R =
  V \cdot
  \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & -1
  \end{bmatrix} \cdot U^T.
\end{equation*}

Таким образом, искомая матрица поворота $R$ имеет вид  \cite{svd}
\begin{equation*}
  R =
  V \cdot
  \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & \det{ \left( V \cdot U^T \right) }
  \end{bmatrix} \cdot U^T,
\end{equation*}
а оптимальный вектор сдвига вычисляется по формуле \eqref{eq:translation}

Алгоритм состоит в поочерёдном выполнении двух шагов:
\begin{enumerate}
  \item поиск наилучшей разметки $k$ при фиксированных $R$ и $b$;
  \item поиск матрицы поворота $R$ и вектора сдвига $ \vec{b}$ при фиксированной разметке $k$,
\end{enumerate}
пока не будет достигнут минимум в \eqref{eq:least:squares}.
Таким образом, имеем покоординатный спуск,
который останавливается в стационарной точке и не гарантирует достижения даже локального минимума.
Метод имеет ограниченное применение и хорошо работает только в том случае,
если шум, а также начальный угол поворота и сдвиг можества $S$ относительно множества $T$ малы.
На рисунке \ref{fig:triangles} изображён пример двух множеств,
для которых алгоритм не даёт ожидаемого результата.

\begin{figure}[h]
  \centering
  \includestandalone[mode=buildnew]{../tikz/triangles}
  \caption{Множества, для которых ICP не даёт ожидаемого результата}
  \label{fig:triangles}
\end{figure}

Множества представляют собой два идентичных треугольника,
отличающихся углом поворота $ \pi $ и смещением
\begin{equation*}
  \vec{b} =
  \begin{bmatrix}
    \frac{ \sqrt{2}}{2} \\
    \frac{ \sqrt{2}}{2}
  \end{bmatrix}
\end{equation*}
Множество $T$ изображено пунктиром.
Это треугольник с вершинами
$ \left( 0, 0 \right), \left( 0, 1 \right) $ и $ \left( 1, 0 \right) $.
Множество $S = R \cdot \vec{t} + \vec{b}$~---~сплошной линией,
где $ \vec{t}$~---~точки множетсва $T$,
а матрица поворота
\begin{equation}
  R =
  \begin{bmatrix}
    \cos \theta & -\sin \theta \\
    \sin \theta & \cos \theta
  \end{bmatrix} =
  \begin{bmatrix}
    -1 & 0 \\
    0 & -1
  \end{bmatrix}.
\end{equation}
Таким образом, множества $T$ и $S$ имеют вид
\begin{equation*}
  T =
  \begin{bmatrix}
    0 & 0 & 1 \\
    0 & 1 & 0
  \end{bmatrix}, \,
  S =
  \begin{bmatrix}
    \frac{ \sqrt{2}}{2} & \frac{ \sqrt{2}}{2} & \frac{ \sqrt{2}}{2} - 1 \\
    \frac{ \sqrt{2}}{2} & \frac{ \sqrt{2}}{2} - 1 & \frac{ \sqrt{2}}{2}
  \end{bmatrix}.
\end{equation*}

Считаем расстояния от каждой точки из множества $S$ до каждой точки из множества $T$
\begin{gather*}
  d \left( \vec{s}_1, \vec{t}_1 \right) = \sqrt{ \left( \frac{ \sqrt{2}}{2} - 0 \right)^2 + \left( \frac{ \sqrt{2}}{2} - 0 \right)^2} = 1, \\
  d \left( \vec{s}_1, \vec{t}_2 \right) = \sqrt{ \left( \frac{ \sqrt{2}}{2} - 0 \right)^2 + \left( \frac{ \sqrt{2}}{2} - 1 \right)^2} \approx 0.77, \\
  d \left( \vec{s}_1, \vec{t}_3 \right) = \sqrt{ \left( \frac{ \sqrt{2}}{2} - 1 \right)^2 + \left( \frac{ \sqrt{2}}{2} - 0 \right)^2} \approx 0.77, \\
  d \left( \vec{s}_2, \vec{t}_1 \right) = \sqrt{ \left( \frac{ \sqrt{2}}{2} - 0 \right)^2 + \left( \frac{ \sqrt{2}}{2} - 1 - 0 \right)^2} \approx 0.77, \\
  d \left( \vec{s}_2, \vec{t}_2 \right) = \sqrt{ \left( \frac{ \sqrt{2}}{2} - 0 \right)^2 + \left( \frac{ \sqrt{2}}{2} - 1 - 1 \right)^2} \approx 1.47, \\
  d \left( \vec{s}_2, \vec{t}_3 \right) = \sqrt{ \left( \frac{ \sqrt{2}}{2} - 1 \right)^2 + \left( \frac{ \sqrt{2}}{2} - 1 - 0 \right)^2} \approx 0.41, \\
  d \left( \vec{s}_3, \vec{t}_1 \right) = \sqrt{ \left( \frac{ \sqrt{2}}{2} - 1 - 0 \right)^2 + \left( \frac{ \sqrt{2}}{2} - 0 \right)^2} \approx 0.77, \\
  d \left( \vec{s}_3, \vec{t}_2 \right) = \sqrt{ \left( \frac{ \sqrt{2}}{2} - 1 - 0 \right)^2 + \left( \frac{ \sqrt{2}}{2} - 1 \right)^2} \approx 0.41, \\
  d \left( \vec{s}_3, \vec{t}_3 \right) = \sqrt{ \left( \frac{ \sqrt{2}}{2} - 1 - 1 \right)^2 + \left( \frac{ \sqrt{2}}{2} - 0 \right)^2} \approx 1.47.
\end{gather*}

Выбираем минимальные расстояния для каждой точки $ \vec{s} \in S$.
Таким образом, разметка имеет вид
\begin{equation*}
  \vec{k}_{s_1} = \vec{t_2}, \,
  \vec{k}_{s_2} = \vec{t_3}, \,
  \vec{k}_{s_3} = \vec{t_2}.
\end{equation*}

Находим
\begin{gather*}
  \overline{s} = \frac{1}{3}
  \begin{bmatrix}
    \frac{ \sqrt{2}}{2} + \frac{ \sqrt{2}}{2} + \frac{ \sqrt{2}}{2} - 1 \\
    \frac{ \sqrt{2}}{2} + \frac{ \sqrt{2}}{2} - 1 + \frac{ \sqrt{2}}{2}
  \end{bmatrix} =
  \begin{bmatrix}
    0.37 \\
    0.37
  \end{bmatrix}, \\
  \overline{k}_s = \frac{1}{3}
  \begin{bmatrix}
    0 + 1 + 0 \\
    1 + 0 + 1
  \end{bmatrix} =
  \begin{bmatrix}
    0.33 \\
    0.67
  \end{bmatrix}.
\end{gather*}
Находим матрицы
\begin{gather*}
  \tilde{S} =
  \begin{bmatrix}
    \frac{ \sqrt{2}}{2} - \frac{ \sqrt{2}{2}} + \frac{1}{3} & \frac{ \sqrt{2}}{2} - \frac{ \sqrt{2}}{2} + \frac{1}{3} & \frac{ \sqrt{2}}{2} - 1 - \frac{ \sqrt{2}}{2} + \frac{1}{3} \\
    \frac{ \sqrt{2}}{2} - \frac{ \sqrt{2}}{2} + \frac{1}{3} & \frac{ \sqrt{2}}{2} - 1 - \frac{ \sqrt{2}}{2} + \frac{1}{3} & \frac{ \sqrt{2}}{2} - \frac{ \sqrt{2}}{2} + \frac{1}{3}
  \end{bmatrix} \approx \\
  \approx \begin{bmatrix}
    0.33 & 0.33 & -0.67 \\
    0.33 & -0.67 & 0.33
  \end{bmatrix}, \\
  \tilde{K}_s =
  \begin{bmatrix}
    0 - \frac{1}{3} & 1 - \frac{1}{3} & 0 - \frac{1}{3} \\
    1 - \frac{2}{3} & 0 - \frac{2}{3} & 1 - \frac{2}{3}
  \end{bmatrix} \approx
  \begin{bmatrix}
    -0.33 & 0.67 & -0.33 \\
    0.33 & -0.67 & 0.33
  \end{bmatrix}.
\end{gather*}
Вычисляем матрицу $X$ и берём от неё сигнулярное разложение
\begin{gather*}
  X = \tilde{S} \cdot \tilde{K}_s^T =
  \begin{bmatrix}
    0.33 & -0.33 \\
    -0.67 & 0.67
  \end{bmatrix} = \\
  = \begin{bmatrix}
    -0.4472136 & 0.89442719 \\
    0.89442719 & 0.4472136
  \end{bmatrix} \cdot
  \begin{bmatrix}
    1.05409255 & 0 \\
    0 & 0
  \end{bmatrix} \cdot
  \begin{bmatrix}
    -0.70710678 & -0.70710678 \\
    -0.70710678 & -0.70710678
  \end{bmatrix} =
  U \cdot \Sigma \cdot V^T.
\end{gather*}
Вычисляем матрицу поворота
\begin{equation*}
  R = V \cdot U^T
  \begin{bmatrix}
    0.9486833 & -0.31622777 \\
    0.31622777 & 0.9486833
  \end{bmatrix}.
\end{equation*}
Определитель этой матрицы равен единице, поэтому $R$~---~матрица поворота.
Находим смещение
\begin{equation*}
  \vec{b} =
  \overline{k}_s - R \cdot \overline{s} =
  \begin{bmatrix}
    0.09693825 \\
    0.1938765
  \end{bmatrix}.
\end{equation*}
Находим новое множество $S$
\begin{equation*}
  S =
  \begin{bmatrix}
    0.54 & 0.86 & -0.4 \\
    1.09 & 0.14 & 0.77
  \end{bmatrix}.
\end{equation*}
Новая разметка совпадает со старой, значит, можем завершить вычисления.
Полученные взаимные размещения множеств изображены на рисунке \ref{fig:triangles_result}


\begin{figure}[h]
  \centering
  \includestandalone[mode=buildnew]{../tikz/triangles_result}
  \caption{Результат ICP для треугольников}
  \label{fig:triangles_result}
\end{figure}

Правильным результатом были бы такие поворот и смещение,
когда все соответствующие точки двух треугольников имели бы одинаковое положение.
Из-за того, что ближайшие точки не являются соответствующими, глобальный минимум не достигается.

\subsection{Итеративный алгоритм ближайших точек с нормалями}

Отличием данного алгоритма (Normal ICP) \cite{nicp} является то,
что он рассматривает каждую точку вместе с локальными особенностями поверхности
\begin{equation*}
  E \left( k, R, b \right) =
  \sum \limits_{s \in S}
  \alpha_{point} \cdot \left \Vert \vec{k}_s - R \cdot \vec{s} - \vec{b} \right \Vert^2 +
  \alpha_{plane} \cdot
    \left|
      \vec{n}_s^T \cdot \left( \vec{k}_s - R \cdot \vec{s} - \vec{b} \right)
    \right|
  \to \min \limits_{k, R, b},
\end{equation*}
где $ \alpha_{point}$ и $ \alpha_{plane}$~---~константы,
а $ \vec{n}_s$~---~нормаль к точке $ \vec{s}$ на исходном облаке.
Для улучшения работы алгоритма убираются
\begin{enumerate}
  \item вершины,
        нормали которых слишком отличаются от нормалей ближайших соседей из целевого облака;
  \item вершины, которые находятся далеко от соседей из целевого облака;
  \item вершины, которые находятся на краю объектов.
\end{enumerate}
